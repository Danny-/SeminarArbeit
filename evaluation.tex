Im Paper \NaNg\cite{paper:Na} wurden noch verschiedene Performancetests durchgeführt. Dafür wurde die TREC Blog Sammlung verwendet. Die meisten Suchanfragen darin sind Eigennamen. Davon sind 33 Personenentitäten und 108 Objekte. Die restlichen 9 Suchanfragen sind allgemeiner Natur. Von den 150 Queries wurden 50 Suchanfragen als Trainingsmenge zum Parametertuning verwendet und die restlichen 100 Queries zum Testen.\\
Die Support Vector Machine Klassifikatoren wurden ebenfalls mit 50 Queries eingestellt. Damit konnten von den verbleibenden 100 Suchanfragen 93 richtig einem Objekt oder einer Person zugeordnet werden. Von den 7 falsch klassifizierten Suchanfragen wurden 4 Objekte als Personen erkannt und 3 Personen als Objekte.\\
Zuletzt wurden verschiedene Testläufe mit verschiedenen Modellen gestartet:
\begin{itemize}
	\item \textbf{REF}: Hierfür wurde die raw entity frequency im vorgestellten Retrieval Modell verwendet.
	\item \textbf{CEEF-Thr}: Das Ranking wurde mit dem Coreferntial Enhanced Entity Frequency Modell nach dem threshold Ansatz durchgeführt.
	\item \textbf{CEEF-2Poisson}: Ranking mit CEEF-Modell. Die möglichen Koreferenzen beschränkten sich dabei auf die Mengen\\
$A_P=\left\{ \text{'he', 'she', 'his', 'her', 'himself', 'herself'} \right\}$ und
$A_0=\left\{ \text{'it', 'its'} \right\}$.
	\item \textbf{CEEF-FB-2Poisson}: Die möglichen Koreferenzen wurden mit dem unter ``Automatische Erkennung von Koreferenzen'' vorgestellten Verfahren von Seung-Hoon Na und Hwee Tou Ng ermittelt. Mit den so erhaltenen Mengen an Koreferenzen wurde das CEEF-Modell angewandt.
	\item \textbf{CEEF-FB-2Poisson*}: Hier wurde das CEEF-Model mit der ``Feedbackbased Identification'' verwendet. Allerdings wurden die Entitäten nicht mit dem Support Vector Machine Klassifikator Personen oder Objekten zugeordnet, sondern per Hand.
\end{itemize}
\begin{table}[h]
	\centering
	\begin{tabular}{lS[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]S[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]S[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]}
		\toprule
		Methods & {MAP} & {Pr@5} & {Pr@10}\\
		\midrule
		REF & 0.3929 & 0.6707 & 0.6616\\
		CEEF-Thr & 0.3968 & 0.6788 & 0.6778\\
		CEEF-2Poisson & 0.4139 & 0.7060 & 0.6940\\
		CEEF-FB-2Poisson & 0.4165 & 0.7320 & 0.7200\\
		CEEF-FB-2Poisson* & 0.4177 & 0.7420 & 0.7270\\
		\bottomrule
	\end{tabular}
	\caption{Comparison of MAP, Pr@5, Pr@10}
	\cite{paper:Na}
\end{table}\\
\\
In der ersten Tabelle sieht man die einzelnen Methoden im direkten Vergleich. Als Testmaß verwendete man MAP, die Mean Average Precision, Pr@5, die Präzision für die besten 5 Dokumente und Pr@10, die Präzision für die besten 10 Dokumente.



\begin{table}[h]
	\centering
	\begin{tabular}{lS[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]S[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]}
		\toprule
		Methods & {LM} & {LMFB} \\
		\midrule
		B (Baseline Run) & 0.4043 & 0.4254 \\
		B+REF & 0.4665 & 0.4868 \\
		B+CEEF-Thr & 0.4831 & 0.4942 \\
		B+CEEF-2Poisson & 0.4874 & 0.5000 \\
		B+CEEF-FB-2Poisson & 0.5074 & 0.5090 \\
		B+CEEF-FB-2Poisson* & 0.5087 & 0.5106 \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of MAP of REF and CEEF methods combining with Language Models}
	\cite{paper:Na}
\end{table}\\
\\




\begin{table}[h]
	\centering
		\begin{tabular}{lS[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]S[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]S[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]S[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]}
			\toprule
			Query Type & {REF} & {CEEF} & {CEEF-FB} & {CEEF-FB*}\\
			\midrule
			Person & 0.5640 & 0.5793 & 0.5761 & 0.5781\\
			Company, etc. & 0.5061 & 0.5170 & 0.5305 & 0.5305\\
			Product & 0.5245 & 0.5401 & 0.5680 & 0.5680\\
			TV program, etc & 0.3664 & 0.3754 & 0.3793 & 0.3994\\
			Organization & 0.4468 & 0.4682 & 0.4701 & 0.4701\\
			Event, etc. & 0.5039 & 0.4987 & 0.5012 & 0.5021\\
			Other Types & 0.4785 & 0.4889 & 0.5040 & 0.5040\\
			Topic & 0.2754 & 0.3078 & 0.3094 & 0.3123\\
			All & 0.4868 & 0.5000 & 0.5090 & 0.5106\\
			\bottomrule
		\end{tabular}
		\caption{Comparison of MAP of REF and CEEF methods, combining with LMFB each query type.\cite{paper:Na}}
\end{table}\\
\\








\begin{table}[h]
	\centering
	\begin{tabular}{lS[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]S[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]S[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]S[tabnumalign=centre,tabformat=1.4,decimalsymbol=comma]}
		\toprule
		Query Type & {NONE} & {REF} & {CEEF} & {CEEF-FB}\\
		\midrule
		TREC '07 & 0.5406 & 0.5569 & 0.5646 & 0.5733\\
		TREC '08 & 0.5032 & 0.5216 & 0.5248 & 0.5330\\
		\bottomrule
	\end{tabular}
	\caption{Comparison of MAP of TREC08BEST and others. NONE means non-combined baseline run.}
	\cite{paper:Na}
\end{table}\\
\\
